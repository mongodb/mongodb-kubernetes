#!/usr/bin/env bash

set -Eeou pipefail

source scripts/funcs/checks
source scripts/funcs/errors
source scripts/funcs/printing

ensure_namespace() {
    local namespace="${1}"
    local tmp_file
    tmp_file=$(mktemp)
    cat <<EOF > "${tmp_file}"
apiVersion: v1
kind: Namespace
metadata:
  name: ${namespace}
  labels:
    evg: task
  annotations:
    evg/version: "https://evergreen.mongodb.com/version/${version_id:-'not-specified'}"
    evg/task-name: ${TASK_NAME:-'not-specified'}
    evg/task: "https://evergreen.mongodb.com/task/${task_id:-'not-specified'}"
    evg/mms-version: "${ops_manager_version:-'not-specified'}"
EOF

    if kubectl get "ns/${namespace}" -o name &> /dev/null; then
        echo "Namespace ${namespace} already exists!"
    else
        echo "Creating new namespace: ${namespace}"
        cat "${tmp_file}"
        kubectl create -f "${tmp_file}"
    fi
}

delete_operator() {
    local ns="$1"
    local name=${OPERATOR_NAME:=mongodb-kubernetes-operator}

    title "Removing the Operator deployment ${name}"
    kubectl delete deployment "${name}" -n "${ns}" --wait=true --timeout=10s|| true
}

# wait_for_operator waits for the Operator to start
wait_for_operator_start() {
    local ns="$1"
    local timeout="${2:-2m}"
    echo "Waiting until the Operator gets to Running state..."
    export OPERATOR_NAME

    local cmd

    # Waiting until there is only one pod left as this could be the upgrade operation (very fast in practice)
    # shellcheck disable=SC2016
    cmd='while [[ $(kubectl -n '"${ns}"' get pods -l app.kubernetes.io/name=${OPERATOR_NAME}  --no-headers 2>/dev/null | wc -l) -gt 1 ]] ; do printf .; sleep 1; done'
    timeout --foreground "1m" bash -c "${cmd}" || true

    cmd="while ! kubectl -n ${ns} get pods -l app.kubernetes.io/name=${OPERATOR_NAME} -o jsonpath={.items[0].status.phase} 2>/dev/null | grep -q Running ; do printf .; sleep 1; done"
    timeout --foreground "${timeout}" bash -c "${cmd}" || true

    # In the end let's check again and print the state
    if ! kubectl -n "${ns}" get pods -l "app.kubernetes.io/name=${OPERATOR_NAME}" -o jsonpath="{.items[0].status.phase}" | grep -q "Running"; then
        error "Operator hasn't reached RUNNING state after ${timeout}. The full yaml configuration for the pod is:"
        kubectl -n "${ns}" get pods -l "app.kubernetes.io/name=${OPERATOR_NAME}" -o yaml

        title "Operator failed to start, exiting"
        return 1
    fi
    echo ""

    title "The Operator successfully installed to the Kubernetes cluster"
}

# recreates docker credentials secret in the cluster
create_image_registries_secret() {
  echo "Started creating image-registries-secret in cluster(s)"

  if ! kubectl cluster-info > /dev/null 2>&1; then
    echo -e "\033[31mCannot get cluster info - does the cluster exist and is reachable?"
    exit 0
  fi

  secret_name="image-registries-secret"

  if [[ "${NAMESPACE:-""}" == "" ]]; then
    echo -e "\033[31mNAMESPACE env var is not set, skipping creating image-registries-secret"
    exit 0
  fi

  if which kubectl > /dev/null; then
      echo "Kubectl command is there, proceeding..."
  else
      echo -e "\033[31mKubectl doesn't exist, skipping setting the context"
      exit 0
  fi

  create_pull_secret() {
    context=$1
    namespace=$2
    secret_name=$3

    # Detect the correct config file path based on container runtime
    local config_file
    local temp_config_file=""
    if command -v podman &> /dev/null && (podman info &> /dev/null || sudo podman info &> /dev/null); then
      # For Podman, use root's auth.json since minikube uses sudo podman
      config_file="/root/.config/containers/auth.json"
      echo "Using Podman config: ${config_file}"

      # Create a temporary copy that the current user can read
      temp_config_file=$(mktemp)
      sudo cp "${config_file}" "${temp_config_file}"
      sudo chown "$(whoami):$(whoami)" "${temp_config_file}"
      config_file="${temp_config_file}"
    else
      # For Docker, use standard docker config
      config_file="${HOME}/.docker/config.json"
      echo "Using Docker config: ${config_file}"
    fi

    # shellcheck disable=SC2154
    if kubectl --context "${context}" get namespace "${namespace}"; then
      kubectl --context "${context}" -n "${namespace}" delete secret "${secret_name}" --ignore-not-found
      echo "${context}: Creating ${namespace}/${secret_name} pull secret"
      kubectl --context "${context}" -n "${namespace}" create secret generic "${secret_name}" \
        --from-file=.dockerconfigjson="${config_file}" --type=kubernetes.io/dockerconfigjson
    else
      echo "Skipping creating pull secret in ${context}/${namespace}. The namespace doesn't exist yet."
    fi

    # Clean up temporary file
    if [[ -n "${temp_config_file}" ]] && [[ -f "${temp_config_file}" ]]; then
      rm -f "${temp_config_file}"
    fi
  }

  echo "Creating/updating pull secret from docker configured file"
  if [[ "${KUBE_ENVIRONMENT_NAME:-}" == "multi" ]]; then
      create_pull_secret "${CENTRAL_CLUSTER}" "${NAMESPACE}" "${secret_name}" &
      for member_cluster in ${MEMBER_CLUSTERS}; do
        for ns in ${WATCH_NAMESPACE//,/ }; do
          create_pull_secret "${member_cluster}" "${ns}" "${secret_name}" &
        done
      done
      wait
  else
    current_ctx=$(kubectl config current-context)
    create_pull_secret "${current_ctx}" "${NAMESPACE}" "${secret_name}" &
    for ns in ${WATCH_NAMESPACE//,/ }; do
      if [[ "${ns}" == "${NAMESPACE}" ]]; then
        continue
      fi
      create_pull_secret "${current_ctx}" "${ns}" "${secret_name}" &
    done
    wait
  fi
}

force_delete_all_resources_from_namespace() {
    resource_type="$1"
    namespace="$2"

    echo "Attempting normal deletion of ${resource_type} in ${namespace}..."
    kubectl delete "${resource_type}" --all -n "${namespace}" --wait=true --timeout=10s || true

    # Check if any resources are still stuck
    echo "Checking if any resources are still stuck:"
    kubectl get "${resource_type}" -n "${namespace}" --no-headers -o custom-columns=":metadata.name" || true
    resources=$(kubectl get "${resource_type}" -n "${namespace}" --no-headers -o custom-columns=":metadata.name" 2>/dev/null || true)

    for resource in ${resources}; do
        echo "${resource_type}/${resource} is still present, force deleting..."

        kubectl patch "${resource_type}" "${resource}" -n "${namespace}" -p '{"metadata":{"finalizers":null}}' --type=merge || true
        kubectl delete "${resource_type}" "${resource}" -n "${namespace}" --force --grace-period=0 || true
    done
}

reset_namespace() {
  context=$1
  namespace=$2
  if [[ "${context}" == "" ]]; then
    echo "context cannot be empty"
    exit 1
  fi

  set +e

  # Cleans the namespace. Note, that fine-grained cleanup is performed instead of just deleting the namespace as it takes
  # considerably less time
  title "Cleaning Kubernetes resources in context: ${context}"

  ensure_namespace "${namespace}"

  force_delete_all_resources_from_namespace "mdb" "${namespace}"
  force_delete_all_resources_from_namespace "mdbu" "${namespace}"
  force_delete_all_resources_from_namespace "mdbc" "${namespace}"
  force_delete_all_resources_from_namespace "mdbmc" "${namespace}"
  force_delete_all_resources_from_namespace "om" "${namespace}"
  force_delete_all_resources_from_namespace "clustermongodbroles" "${namespace}"

  echo "Sleeping to allow the operator to perform cleanups"
  sleep 10
  helm uninstall --kube-context="${context}" mongodb-kubernetes-operator || true &
  helm uninstall --kube-context="${context}" mongodb-kubernetes-operator-multi-cluster || true &

  # Openshift variant runs all tests sequentially. In order to avoid clashes between tests, we need to wait till
  # the namespace is gone. This trigger OpenShift Project deletion, which is a "Namespace on Steroids" and it takes
  # a while to delete it.
  should_wait="false"
  # shellcheck disable=SC2153
  if [[ ${KUBE_ENVIRONMENT_NAME} == "openshift" ]]; then
    should_wait="true"
    echo "Removing the test namespace ${namespace}, should_wait=${should_wait}"
    kubectl --context "${context}" delete "namespace/${namespace}" --wait="${should_wait}" || true
  fi

  echo "Removing CSRs"
  kubectl --context "${context}" delete "$(kubectl get csr -o name | grep "${NAMESPACE}")" &> /dev/null || true
}

delete_kind_network() {
  if ! docker network ls | grep -q "kind"; then
     echo "Docker network 'kind' does not exist."
     return 0
  fi
  echo "Docker network 'kind' exists."

  # Stop all containers in the "kind" network
  containers=$(docker network inspect -f '{{range .Containers}}{{.Name}} {{end}}' kind)
  if [[ -n "${containers}" ]]; then
      echo "Stopping containers ${containers} using kind network..."
      # shellcheck disable=SC2086
      docker stop ${containers}
      docker container prune -f
  fi

  docker network rm kind
}

docker_run_local_registry() {
  reg_name="$1"
  reg_port="$2"
  running="$(docker inspect -f '{{.State.Running}}' "${reg_name}" 2>/dev/null || true)"
  if [ "${running}" != 'true' ]; then
    docker run -d --restart=always -p "127.0.0.1:${reg_port}:5000" --name "${reg_name}" registry:2
  fi
}

docker_create_kind_network() {
  # We create docker network primarily so that all kind clusters use the same network.
  # We hardcode 172.18/16 subnet in few places, so we must ensure the network uses that subnet:
  #   - we set IP ranges for MetalLB
  #   - we write into coredns config map with IP used for exposing pods externally and for no-mesh test
  # In case of any errors, verify if any other subnet is not already using that subnet.
  docker network create --subnet=172.18.0.0/16 kind || true
}

docker_network_connect_to_kind() {
# connect the registry to the cluster network if not already connected
  if [ "$(docker inspect -f='{{json .NetworkSettings.Networks.kind}}' "${reg_name}")" = 'null' ]; then
    docker network connect "kind" "${reg_name}"
  fi
}

# Function to run a script with kubectl wrapped to use a specific context.
# This is useful when we run third-party/downloaded scripts which don't specify kubectl context
# explicitly but assume kubectl works on the current context.
# When kubectl is assuming current context it is not possible to execute it concurrently for different contexts.
run_script_with_wrapped_kubectl() {
    local script_path="$1"
    local context="$2"

    # Create a temporary wrapper script in the same directory as the script to run
    local script_dir
    script_dir="$(dirname "${script_path}")"
    local wrapper_script="${script_dir}/kubectl_wrapper_${context//\//_}_$$.sh"

    cat > "${wrapper_script}" << EOF
#!/bin/bash
# Define kubectl function to include the context
kubectl() {
  command kubectl --context "${context}" "\$@"
}
export -f kubectl
source "${script_path}"
EOF

    # Make the wrapper script executable
    chmod +x "${wrapper_script}"

    # Run the wrapper script
    echo "Running script ${script_path} with kubectl wrapped for context ${context}"
    local result=0
    if ! "${wrapper_script}"; then
        echo "Failed to run ${script_path} with kubectl wrapped for context ${context}"
        result=1
    fi

    # Clean up the temporary script
    rm "${wrapper_script}"

    return ${result}
}
